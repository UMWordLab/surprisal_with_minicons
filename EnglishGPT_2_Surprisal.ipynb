{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UMWordLab/surprisal_with_minicons/blob/main/EnglishGPT_2_Surprisal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2 English Surprisals for Experimental Stimuli (CSV or single sentence)"
      ],
      "metadata": {
        "id": "g7AFHTk7ujE4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUXWY3AvsM0s"
      },
      "source": [
        "This script was created by [Yizhi Tang](https://github.com/tangyizhi2000) and [Lisa Levinson](https://lisalevinson.github.io/) for use in the [WordLab](https://umwordlab.github.io/), and uses the [Minicons](https://github.com/kanishkamisra/minicons) package by [Kanishka Misra](https://kanishka.website/).\n",
        "\n",
        "The purpose of this notebook is to calculate lexical surprisal (for English) using GPT-2.\n",
        "\n",
        "We use Minicons, a python library that automates the probability computations of transformer LMs that are accessible through the transformers package by HuggingFace.\n",
        "\n",
        "One challenge in matching GPT-2 surprisals with experimental stimuli is the tokenization of GPT-2, which uses BPE (byte-pair encoding). For example, GPT-2 tokenizes the word \"inflating\" into two separate tokens: \"infl\" and \"ating\", and Minicons gives the probability of two tokens respectively. To compute the full word surprisal as a match to a word in the stimulus, we combine (by addition, due to logarithmic values) the probabilities of sub-words to estimate the surprisal of the target word.\n",
        "\n",
        "Note that puncuation is evaluated, so the surprisal will be different if, for example, you include the period at the end of the sentence or do not. The current version of this script combines (same as other BPE combinations) with punctuation with the previous word if they are not separated with a space. If you are analyzing words at a sentence point with puncuation, you should consider carefully whether you want the surprisal for the word alone, or the word with  punctuation which may entail much more about the prosody and syntax. \n",
        "\n",
        "You can use different GPT-2 models by changing the model name in the setup code. Only English has currently been tested for this specific script currently, but it should work for other languages where the orthography breaks up words using spaces.  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Code"
      ],
      "metadata": {
        "id": "VuSqzNmHu6v7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOmzVIwlKegU"
      },
      "outputs": [],
      "source": [
        "# Minicons Installation\n",
        "# Introduction can be found https://kanishka.xyz/post/minicons-running-large-scale-behavioral-analyses-on-transformer-lms/\n",
        "# Tutorial and code can be found https://github.com/kanishkamisra/minicons/blob/master/examples/surprisals.md\n",
        "!pip install minicons\n",
        "# Import necessary libararies\n",
        "from minicons import scorer\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import json\n",
        "import csv\n",
        "# Download GPT-2\n",
        "# Note that HuggingFace GPT-2 has several version (differ in size)\n",
        "# we can replace 'gpt2' with 'gpt2-medium', 'gpt2-large', or 'gpt2-xl'\n",
        "# See https://huggingface.co/transformers/model_doc/gpt2.html#gpt2model for details\n",
        "model = scorer.IncrementalLMScorer('gpt2-medium', 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CSV Input"
      ],
      "metadata": {
        "id": "SDxuTo0FvJXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following block of code takes in a list of sentences and outputs a csv file containing surprisal values. It may take some time for the output file to appear in the left \"files\" panel after the cell has finished running. \n",
        "\n",
        "**input_file**\n",
        "\n",
        "input_file is a file containing the list of sentences in comma separated (csv) format. Each row should contain two columns, the first with a sentence label, the second with the full sentence. \n",
        "\n",
        "Note: if there is a [BOM](https://en.wikipedia.org/wiki/Byte_order_mark) at the beginning of the file, this will mangle the label of your first sentence. \n",
        "\n",
        "As an example, the first few lines of your file might look like:\n",
        "\n",
        "\n",
        "```\n",
        "intr0,The balloons were popping in the car outside of the the party\n",
        "tran1,The clown was popping the balloons in the car\n",
        "tran2,The maid was shrinking the laundry in the dryer\n",
        "intr3,The sweaters were shrinking in the dryer at the cleaners\n",
        "tran4,The leak was eroding the pipes at the connection\n",
        "```\n",
        "\n",
        "**dest_file**\n",
        "\n",
        "dest_file is the csv file generated by this code containing surprisal values. dest_file is in \"long\" format and contains four columns: label, word, word_number, surprisal\n",
        "\n",
        "For example, the first few lines of our dest_file for the sentences above would look like this:\n",
        "\n",
        "```\n",
        "intr0,The,0,0.0\n",
        "intr0,balloons,1,12.588163375854492\n",
        "intr0,were,2,2.1027908325195312\n",
        "intr0,popping,3,8.180717468261719\n",
        "intr0,in,4,3.451995849609375\n",
        "intr0,the,5,1.3590087890625\n",
        "intr0,car,6,6.585639953613281\n",
        "intr0,outside,7,5.377586364746094\n",
        "intr0,of,8,2.3132476806640625\n",
        "intr0,the,9,0.9443435668945312\n",
        "intr0,the,10,7.9730224609375\n",
        "intr0,party,11,5.788078308105469\n",
        "tran1,The,0,0.0\n",
        "tran1,clown,1,11.388120651245117\n",
        "tran1,was,2,2.9219207763671875\n",
        "```"
      ],
      "metadata": {
        "id": "S1NjO8tkqKvU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6atuYFcsiMm"
      },
      "outputs": [],
      "source": [
        "# upload your csv to in the files tab and change the filename here as needed\n",
        "input_file = '/content/input.csv'\n",
        "file = open(input_file, 'r')\n",
        "\n",
        "# rename the output file if desired\n",
        "dest_file = '/content/output.csv'\n",
        "\n",
        "with open(dest_file, mode='w') as csvfile:\n",
        "  csv_file = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "  # Write the first line of dest_file\n",
        "  csv_file.writerow(['label', 'word', 'word_number', 'surprisal'])\n",
        "  for line in file.readlines():\n",
        "    # Parsing input file\n",
        "    sentence = line[line.find(',')+1:].strip()\n",
        "    label = line[:line.find(',')]\n",
        "    # Minicons can take two sentences at once\n",
        "    # Since we are processing sentences one by one, we use a 'placeholder' at the second position\n",
        "    input = [sentence, 'placeholder']\n",
        "    # Use Minicons to calculate surprisal / log likelihood\n",
        "    surprisal = model.token_score(input, surprisal=True, base_two=False)\n",
        "    sentence = sentence.split()\n",
        "    word_number = 0\n",
        "    prev_word = ''\n",
        "    prev_surprisal = 0\n",
        "    # the loop here combines multiple sub-words together and calculate the \n",
        "    # final surprisal value for each word\n",
        "    for word in surprisal[0]:\n",
        "      prev_word += word[0]\n",
        "      prev_surprisal += word[1]\n",
        "      if prev_word == sentence[word_number]:\n",
        "        row = [label, prev_word, word_number, prev_surprisal]\n",
        "        word_number += 1\n",
        "        csv_file.writerow(row)\n",
        "        prev_word = ''\n",
        "        prev_surprisal = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single Sentence Input"
      ],
      "metadata": {
        "id": "NmzQsObhvPXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code block below will just work on one sentence at a time, but otherwise calculates surprisal in the same way as above. "
      ],
      "metadata": {
        "id": "ukizL43JremR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EImcgeixo0Zl"
      },
      "outputs": [],
      "source": [
        "# Takes in a sentence, and output the surprisal values for each word\n",
        "# This function is just a small version of the code above\n",
        "# sometimes we don't have an entire list of stimulus,\n",
        "# instead, we only want a quick check of the surprisal of one sentence\n",
        "def calculate_surprisal(sentence):\n",
        "  input = [sentence, 'placeholder']\n",
        "  # token_score() function of Minicons takes in several parameters\n",
        "  # if surprisal=True, the output value is surprisal instead of log likelihood\n",
        "  # if base_two=True, the log likelihood will be in base 2\n",
        "  # see Minicons documentations for details\n",
        "  surprisal = model.token_score(input, surprisal=True, base_two=False)\n",
        "  word_number = 0\n",
        "  prev_word = ''\n",
        "  prev_surprisal = 0\n",
        "  sentence = sentence.split()\n",
        "  for word in surprisal[0]:\n",
        "    prev_word += word[0]\n",
        "    prev_surprisal += word[1]\n",
        "    if prev_word == sentence[word_number]:\n",
        "      print(prev_word, prev_surprisal)\n",
        "      word_number += 1\n",
        "      prev_word = ''\n",
        "      prev_surprisal = 0\n",
        "\n",
        "# Example Usage:\n",
        "# As shown in the result, 'infl' and 'ating' are combined as one word\n",
        "sentence = 'The balloon was inflating for 10 minutes'\n",
        "calculate_surprisal(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVD7qr9l-Cyd"
      },
      "outputs": [],
      "source": [
        "calculate_surprisal(\"When did the biker crash yesterday?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BPE Demonstration"
      ],
      "metadata": {
        "id": "ms_FP7cQuzgN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54KfFXS5NEkk"
      },
      "outputs": [],
      "source": [
        "# This block is to demonstrate the tokenization problem of GPT-2\n",
        "# As seen from the result, the word 'inflating' is separated as 'infl' and 'ating'\n",
        "# and each sub-word has its own log likelihood.\n",
        "sentences = ['When did the biker crash yesterday?', 'The float was inflating at the carnival near the church.']\n",
        "model.token_score(sentences, surprisal=True, base_two=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}